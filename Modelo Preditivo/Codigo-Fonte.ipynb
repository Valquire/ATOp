{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2cvRsYV3EdE"
   },
   "source": [
    "# **Construção do ATOp-PredictiveModel**\n",
    "Este notebook organiza o código gerado para a análise da autonomia de uma plataforma naval da Marinha do Brasil, utilizando dados coletados entre 2013 e 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Ambiente e Bibliotecas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "11a65100"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import os\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    VotingRegressor,\n",
    "    StackingRegressor\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, root_mean_squared_error\n",
    "\n",
    "import ImbalancedLearningRegression as iblr\n",
    "import optuna\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, clone\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from itertools import combinations\n",
    "from itertools import permutations\n",
    "import time\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "from scipy.stats import friedmanchisquare\n",
    "import scikit_posthocs as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pVmPdNTyKGmM"
   },
   "source": [
    "## **Integração de Dados Historicos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdgNcg6D-gGj",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Carregamento dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega o dataset SINGRA\n",
    "df_Singra = pd.read_csv('C:/Users/12078956/Documents/Projeto/Dados/SINGRA/RMC_MEIO1.csv',\n",
    "                        sep=',', header=0, parse_dates=['PERIODO'],\n",
    "                        dtype={'QTDE_ITENS_RM': np.float64, 'PRECO_UNITARIO_VENDA': np.float64})\n",
    "# Carrega o dataset RFC\n",
    "df_RFC = pd.read_csv('C:/Users/12078956/Documents/Projeto/Dados/RFC/RFC.csv',\n",
    "                     sep=',', header=0) #, dtype={'MILHAS': np.float64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IcqblFt4YCIm",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Exploração dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcUXOe6FHJbx",
    "outputId": "0d90a070-bebd-4d01-b132-a16c0b993690"
   },
   "outputs": [],
   "source": [
    "# Limpeza e padronização dos dados\n",
    "\n",
    "df_Singra['GRUPO_JURISDICAO'].value_counts()\n",
    "\n",
    "# Remove consumable items outside the scope of work\n",
    "df_Singra = df_Singra.drop(df_Singra.loc[lambda df_GENEROS: ((df_Singra.NOME_PORTUGUES == 'REFEIÇÃO INDIVIDUAL') |\n",
    "                                                              (df_Singra.NOME_PORTUGUES == 'RAÇÃO DE SOBREVIVÊNCIA'))].index, axis=0)\n",
    "\n",
    "# Standardization of the Supply Unit (SU)\n",
    "print('UF antes:')\n",
    "print(df_Singra['UF'].value_counts())\n",
    "print('\\n')\n",
    "\n",
    "# 1 Pack of coffee = 0.5 kilogram\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*0.5 if x.UF == 'PACOTE'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "# change the description\n",
    "df_Singra['UF'] = df_Singra.UF.replace('PACOTE', 'QUILOGRAMA')\n",
    "\n",
    "# 1 Bottle of vegetable oil = 0.9 liter\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*0.9 if x.UF == 'GARRAFA'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "df_Singra['UF'] = df_Singra.UF.replace('GARRAFA', 'LITRO') # change the description\n",
    "\n",
    "# 1 US Quarter Gallon of lubricant = 0.946353 liter\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*0.946353 if x.UF == 'QUARTO DE GALAO AMERICANO'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "# 1 drum of lubricant = 200 liters\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*200 if x.UF == 'TAMBOR'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "\n",
    "# 1 Gallon of lubricant = 3.785412 liters\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*3.785412 if x.UF == 'GALAO'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "\n",
    "# 1 Bucket of lubricant = 20 liters\n",
    "# 1 Bucket of grease = 20 kilograms\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*20 if x.UF == 'BALDE'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "# 1 Can of grease = 5 kilograms\n",
    "df_Singra['QTDE_ITENS_RM'] = df_Singra.apply(lambda x: x.QTDE_ITENS_RM*5 if x.UF == 'LATA'\n",
    "                                             else x.QTDE_ITENS_RM, axis = 1)\n",
    "\n",
    "# Change UF description for Lubricant and Grease\n",
    "df_Singra['UF'] = df_Singra.apply(lambda x: 'LITRO' if x.DESCRICAO_CLG == 'LUBRIFICANTE'\n",
    "                                  else 'QUILOGRAMA' if x.DESCRICAO_CLG == 'GRAXA' else x.UF, axis = 1)\n",
    "\n",
    "print('UF depois:')\n",
    "print(df_Singra['UF'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zOjL6Y4Flo7f"
   },
   "outputs": [],
   "source": [
    "df_CLG = df_Singra.query('GRUPO_JURISDICAO == \"COMBUSTIVEIS\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "3OaI6PR44Mu6",
    "outputId": "61ae2c38-67d3-4dc0-de2f-dc99a29d9cc8"
   },
   "outputs": [],
   "source": [
    "df_CLG['DESCRICAO_CLG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6j3CJYFtIcQ"
   },
   "outputs": [],
   "source": [
    "# Remove consumable items outside the scope of work\n",
    "df_CLG = df_CLG.drop(df_CLG.loc[lambda df_CLG: ((df_CLG.DESCRICAO_CLG == 'COMBUSTÍVEL AVIAÇÃO - QAV-5')\n",
    "                                                | (df_CLG.DESCRICAO_CLG == 'GASOLINA COMUM'))].index, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "ZdQRBuMHte8b",
    "outputId": "2b83437d-8ade-4128-9fb8-5d549cc8a1a9"
   },
   "outputs": [],
   "source": [
    "df_CLG['DESCRICAO_CLG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEMpavyG2hql"
   },
   "outputs": [],
   "source": [
    "# Standardization of the description of CLG types\n",
    "df_CLG['DESCRICAO_CLG'] = df_CLG.DESCRICAO_CLG.replace('ÓLEO DIESEL ESPECIAL - OCMT', 'COMBUSTIVEL')\n",
    "df_CLG['DESCRICAO_CLG'] = df_CLG.DESCRICAO_CLG.replace('ÓLEO DIESEL MARÍTIMO', 'COMBUSTIVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "MODxMCO_4FGb",
    "outputId": "d74e27e5-18d8-406b-d30b-8118ca408f38"
   },
   "outputs": [],
   "source": [
    "df_CLG['DESCRICAO_CLG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "sEbjSVod3L5c",
    "outputId": "9a1105a8-e188-4d56-f30d-3781437b9f6e"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "fig1 = px.scatter(df_CLG, x = df_CLG.PERIODO,\n",
    "                  y = df_CLG.QTDE_ITENS_RM,\n",
    "                  color=df_CLG.DESCRICAO_CLG)\n",
    "fig1.update_layout(\n",
    "    title='Visualização do consumo Combustível, Lubrificante e Graxa',\n",
    "    xaxis_title='Período',\n",
    "    yaxis_title='Quantidade',\n",
    "    legend_title='Categoria',\n",
    ")\n",
    "fig1.show(renderer='png', width=1600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "5fJLb7qMxtEE",
    "outputId": "290bd524-d9ce-4a48-f5c8-9c2ac8a4c918"
   },
   "outputs": [],
   "source": [
    "grupoCLG = df_CLG.groupby('DESCRICAO_CLG')\n",
    "grupoCLG.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "AUVeK_27DQAR",
    "outputId": "3f4cb502-e976-4738-b102-4b6716b73b9f"
   },
   "outputs": [],
   "source": [
    "grupoCLG.get_group('COMBUSTIVEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfK8ZJfgflOC"
   },
   "outputs": [],
   "source": [
    "# Method to transform groups into columns\n",
    "pivot_df_CLG = df_CLG.pivot_table(values='QTDE_ITENS_RM', index='PERIODO', columns='DESCRICAO_CLG', aggfunc='sum')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(pivot_df_CLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pOlGBhAwsYbT"
   },
   "outputs": [],
   "source": [
    "df_GENEROS = df_Singra.query('GRUPO_JURISDICAO == \"SUBSISTENCIA\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1370
    },
    "id": "_q8gbylC4VXy",
    "outputId": "6a84e4a7-d8e4-46d6-8acc-109239d49c72"
   },
   "outputs": [],
   "source": [
    "df_GENEROS['NOME_PORTUGUES'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FMXdGaElgAP"
   },
   "source": [
    "Grupo de alimentos, baseado na pirâmida alimentar brasileira (PHILIPPI et al., 1999)\n",
    "A pirâmide é composta de 8 grupos:\n",
    "\n",
    "1.   Arroz, Pão, Massa, Batata, cassava;\n",
    "2.   Vegetais;\n",
    "1.   Frutas;\n",
    "2.   Carnes;\n",
    "1.   Leite, queijo and yogurte;\n",
    "2.   Feijões and olaginosas;\n",
    "1.   Óleos e gorduras;\n",
    "2.   Açucar e doces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "qiq9N9RikZYj",
    "outputId": "ef840afb-6f2a-489d-f227-7cdc76f9de0b"
   },
   "outputs": [],
   "source": [
    "# 02 categories were included, DRINKS (tea, coffee, etc.) and SEASONING (pepper, salt, etc.)\n",
    "# Even though they are not part of the food pyramid, there were records of consumption of these items\n",
    "GRUPO_ALIMENTO = []\n",
    "\n",
    "for i in df_GENEROS.itertuples():\n",
    "  valor = i.NOME_PORTUGUES\n",
    "\n",
    "  if valor == 'AÇÚCAR REFINADO':\n",
    "    GRUPO_ALIMENTO.append('ACUCARES')\n",
    "\n",
    "  elif valor == 'ÓLEO VEGETAL':\n",
    "    GRUPO_ALIMENTO.append('OLEOS_GORDURAS')\n",
    "\n",
    "  elif valor == 'ARROZ DESCASCADO':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'COXA DE FRANGO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'LEITE INTEGRAL EM PÓ':\n",
    "    GRUPO_ALIMENTO.append('LEITE_QUEIJO')\n",
    "\n",
    "  elif valor == 'FEIJÕES PRETOS SECOS':\n",
    "    GRUPO_ALIMENTO.append('FEIJOES')\n",
    "\n",
    "  elif valor == 'FILE PEITO FRANGO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'GELÉIA':\n",
    "    GRUPO_ALIMENTO.append('ACUCARES')\n",
    "\n",
    "  elif valor == 'SUCO CAJU':\n",
    "    GRUPO_ALIMENTO.append('FRUTAS')\n",
    "\n",
    "  elif valor == 'MOLHO DE TOMATE':\n",
    "    GRUPO_ALIMENTO.append('LEGUMES_VERDURAS')\n",
    "\n",
    "  elif valor == 'FARINHA MANDIOCA':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'CREME DE LEITE':\n",
    "    GRUPO_ALIMENTO.append('LEITE_QUEIJO')\n",
    "\n",
    "  elif valor == 'PATINHO ESPECIAL BOVINO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'AZEITE DE OLIVA':\n",
    "    GRUPO_ALIMENTO.append('OLEOS_GORDURAS')\n",
    "\n",
    "  elif valor == 'BIFE DO ALCATRA COM PICANHA BOVINO SEM OSSO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'SUCO MARACUJA':\n",
    "    GRUPO_ALIMENTO.append('FRUTAS')\n",
    "\n",
    "  elif valor == 'LOMBO DESOSSADO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'ESPAGUETE':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'BISCOITO CRACKER DE TRIGO':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'LOMBO EM PEDAÇOS':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'BISCOITO DE AÇÚCAR':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'CHÃ DESOSSADO SEM MÚSCULO OU PONTA BOVINO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'MACARRÃO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'CACAU EM PÓ PARA BEBIDA':\n",
    "    GRUPO_ALIMENTO.append('ACUCARES')\n",
    "\n",
    "  elif valor == 'PERNIL DESOSSADO E AMARRADO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'BIFE DE CONTRA-FILÉ DO LOMBO BOVINO DESOSSADO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'FILÉ MIGNON COMPLETO DE CARNE BOVINA':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'FILEZINHO PEITO FRANGO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'ACÉM BOVINO':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'BIFES DE CARNE BOVINA':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'BIFE DE FILÉ DE CARNE BOVINA SEM OSSO ESPECIAL':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'SUCO DE UVA ENLATADO':\n",
    "    GRUPO_ALIMENTO.append('FRUTAS')\n",
    "\n",
    "  elif valor == 'FEIJÃO-PINTO SECO':\n",
    "    GRUPO_ALIMENTO.append('FEIJOES')\n",
    "\n",
    "  elif valor == 'ARROZ PARBOILIZADO':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'BIFE BOVINO EM CUBOS ESPECIAL':\n",
    "    GRUPO_ALIMENTO.append('CARNES')\n",
    "\n",
    "  elif valor == 'BEBIDA COM SABOR DE FRUTA':\n",
    "    GRUPO_ALIMENTO.append('FRUTAS')\n",
    "\n",
    "  elif valor == 'ARROZ MARROM':\n",
    "    GRUPO_ALIMENTO.append('ARROZ_MASSA')\n",
    "\n",
    "  elif valor == 'CAFÉ TORRADO':\n",
    "    GRUPO_ALIMENTO.append('BEBIDAS')\n",
    "\n",
    "  else:\n",
    "    GRUPO_ALIMENTO.append('TEMPEROS')\n",
    "\n",
    "# Add Food Group column\n",
    "df_GENEROS.insert(14, 'GRUPO_ALIMENTO', GRUPO_ALIMENTO)\n",
    "\n",
    "df_GENEROS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "54AsNIqFoKmS",
    "outputId": "385eb71d-3b7c-4b1f-dbb3-67b23f12602a"
   },
   "outputs": [],
   "source": [
    "df_GENEROS['GRUPO_ALIMENTO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 517
    },
    "id": "P8TLiF77qhXC",
    "outputId": "e7736dee-e391-427c-a61d-3cc67b9ceb09"
   },
   "outputs": [],
   "source": [
    "fig1 = px.scatter(df_GENEROS, x = df_GENEROS.PERIODO,\n",
    "                  y = df_GENEROS.QTDE_ITENS_RM,\n",
    "                  color=df_GENEROS.GRUPO_ALIMENTO)\n",
    "fig1.update_layout(\n",
    "    title='Visualização do consumo de Alimentos',\n",
    "    xaxis_title='Período',\n",
    "    yaxis_title='Quantidade',\n",
    "    legend_title='Categoria',\n",
    ")\n",
    "fig1.show(renderer='png', width=1600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epNy2ZiQto6B"
   },
   "outputs": [],
   "source": [
    "# Method to transform groups into columns\n",
    "pivot_df_GENEROS = df_GENEROS.pivot_table(values='QTDE_ITENS_RM', index='PERIODO', columns='GRUPO_ALIMENTO', aggfunc='sum')\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# print(pivot_df_GENEROS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "id": "pHjezs6novzz",
    "outputId": "5b774cef-7e7e-40f0-f4b5-feb860cb88bd"
   },
   "outputs": [],
   "source": [
    "grupoGENEROS = df_GENEROS.groupby('GRUPO_ALIMENTO')\n",
    "grupoGENEROS.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "frsy1qocpBPM",
    "outputId": "5cf49f0c-3e78-4261-8f20-8567d1263b44"
   },
   "outputs": [],
   "source": [
    "grupoGENEROS.get_group('FRUTAS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0KlFKtZjsJZ"
   },
   "outputs": [],
   "source": [
    "# Concatenates the CLG and GENEROS dataframes\n",
    "df_Singra_T = pd.concat([pivot_df_CLG, pivot_df_GENEROS], axis=1)\n",
    "# Impute the mean for unknown values\n",
    "df_Singra_T = df_Singra_T.fillna(df_Singra_T.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1mx-l0w9SWJT"
   },
   "outputs": [],
   "source": [
    "# Includes the PERIOD index as a dataframe column\n",
    "df_Singra_T = df_Singra_T.rename_axis('PERIODO').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "h92_nWQFSd16",
    "outputId": "c0bbcd4b-2acc-4102-a1fb-a8569fbed5ee"
   },
   "outputs": [],
   "source": [
    "df_Singra_T.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KDuqZI5PVo5C"
   },
   "outputs": [],
   "source": [
    "# Add the ANO column\n",
    "ano = []\n",
    "for i in df_Singra_T.itertuples():\n",
    "  ano.append(i.PERIODO.year)\n",
    "\n",
    "df_Singra_T.insert(loc=1, column='ANO', value=ano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnHW4xa0Yji4"
   },
   "outputs": [],
   "source": [
    "# Add the TRIMESTRE column\n",
    "trimestre =[]\n",
    "for i in df_Singra_T.itertuples():\n",
    "  if i.PERIODO.month <= 3:\n",
    "    trimestre.append(1)\n",
    "  elif i.PERIODO.month <= 6:\n",
    "    trimestre.append(2)\n",
    "  elif i.PERIODO.month <= 9:\n",
    "    trimestre.append(3)\n",
    "  else:\n",
    "    trimestre.append(4)\n",
    "\n",
    "df_Singra_T.insert(loc=2, column='TRIMESTRE', value=trimestre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "36NJD63aXLfr",
    "outputId": "409ba0a7-a0f0-43e8-a3f0-eec976a146fb"
   },
   "outputs": [],
   "source": [
    "df_Singra_T.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "bhLoNn2UcYgs",
    "outputId": "dba8a2fe-9034-4b04-cd3a-3b895040aa15"
   },
   "outputs": [],
   "source": [
    "# Sums values based on ANO and TRIMESTRE\n",
    "df_SINGRA_TRIM = df_Singra_T.groupby(['ANO', 'TRIMESTRE'])[['COMBUSTIVEL', 'GRAXA',\t'LUBRIFICANTE',\t'ACUCARES',\t'ARROZ_MASSA',\n",
    "                                                            'BEBIDAS',\t'CARNES',\t'FEIJOES',\t'FRUTAS',\t'LEGUMES_VERDURAS',\n",
    "                                                            'LEITE_QUEIJO',\t'OLEOS_GORDURAS',\t'TEMPEROS']].sum()\n",
    "df_SINGRA_TRIM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pts77QssAieC"
   },
   "outputs": [],
   "source": [
    "# # Limpeza do RFC dataset\n",
    "# Eliminates columns for concatenation\n",
    "df_RFC = df_RFC.drop(['TV', 'DT_INICIO', 'DT_FIM', 'OPERACAO', 'PORTOS'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QppJx0pXCRXR",
    "outputId": "dc6f5d05-6628-4b61-b753-a56080742abd"
   },
   "outputs": [],
   "source": [
    "df_RFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmZkvQSkFzGI"
   },
   "outputs": [],
   "source": [
    "# Deletes record columns NaN\n",
    "df_RFC = df_RFC.dropna()\n",
    "df_RFC.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cpqv9QUTGPlS",
    "outputId": "32581e48-d64d-4d22-c701-01a7b2787664"
   },
   "outputs": [],
   "source": [
    "df_RFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELn7u7jFNdMY"
   },
   "outputs": [],
   "source": [
    "# adjust types\n",
    "df_RFC['ANO'] = df_RFC.ANO.astype('int64')\n",
    "df_RFC['TRIMESTRE'] = df_RFC.TRIMESTRE.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8z4DhlhIPyd5",
    "outputId": "66938170-4276-4ac4-c972-67e0c77197d6"
   },
   "outputs": [],
   "source": [
    "df_RFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgo0Cww_KAGN"
   },
   "outputs": [],
   "source": [
    "# Defines the columns ANO and TRIMESTRE as indexes\n",
    "df_RFC = df_RFC.set_index(['ANO', 'TRIMESTRE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "_aubKnBCKQYh",
    "outputId": "7c922339-4eed-4aac-b82f-1194a57aa8fc"
   },
   "outputs": [],
   "source": [
    "df_RFC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UdcD3PiWCmpn"
   },
   "outputs": [],
   "source": [
    "# Concatenates the SINGRA_TRIM and RFC dataframes\n",
    "df_ATOp = pd.concat([df_SINGRA_TRIM, df_RFC], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "C76Ds2-EC6ok",
    "outputId": "294cfc83-6a59-4f63-e850-90f0c6f305ba"
   },
   "outputs": [],
   "source": [
    "df_ATOp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QpnBHQU3TTGr"
   },
   "outputs": [],
   "source": [
    "# Inclui o index ANO e TRIMESTRE como coluna do dataframe\n",
    "df_ATOp = df_ATOp.rename_axis(['ANO', 'TRIMESTRE']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dKCKKwoBWn6a"
   },
   "outputs": [],
   "source": [
    "df_ATOp_rounder = df_ATOp.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "FsTH9HAvFhU3",
    "outputId": "4b73b51f-8662-490f-a8ee-6470a8942b4a"
   },
   "outputs": [],
   "source": [
    "df_ATOp_rounder.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01qLR5J1UXG5"
   },
   "outputs": [],
   "source": [
    "# Removes records outside the study period (2013 to 2017)\n",
    "df_ATOp = df_ATOp.drop(df_ATOp.loc[lambda df_ATOp: df_ATOp.ANO > 2017].index, axis=0)\n",
    "df_ATOp.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "id": "2ECm8-zJWG6X",
    "outputId": "9b0b3a8a-420a-40e2-df2b-05a9fc172778"
   },
   "outputs": [],
   "source": [
    "# Correlation Matrix\n",
    "matriz_correlacao = df_ATOp.corr(method=\"spearman\")\n",
    "matriz_correlacao.style.background_gradient(cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 711
    },
    "id": "QPt6Bwq7x59j",
    "outputId": "4af73c0d-fa5d-4b9a-ad8a-1dc86876770c"
   },
   "outputs": [],
   "source": [
    "df_ATOp.corr(method=\"spearman\").DIAS_MAR.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1fMsMWbAui_o",
    "outputId": "ee90703b-254b-4cc5-82aa-c463e8ced0d7"
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_ATOp[['DIAS_MAR', 'MILHAS', 'DIAS_PORTO_SEDE', 'DIAS_PORTO_FORA_SEDE', 'MILITARES']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkxVhn-XV0AW",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Preparação dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "N-usjXL1jl4B",
    "outputId": "18ce9af9-17ee-49fc-ea52-46197c6a8d5a"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(df_ATOp['DIAS_MAR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZW8rVU_ybMj"
   },
   "outputs": [],
   "source": [
    "# Aplicação da técnica de sobreamostragem sobre os dados reais\n",
    "\n",
    "## specify phi relevance values\n",
    "rg_mtrx_iblr = [\n",
    "\n",
    "    [42,  1, 0],  ## over-sample (\"minority\")\n",
    "    [30, 0, 0],  ## under-sample (\"majority\")\n",
    "    [20, 0, 0],  ## under-sample\n",
    "    [31, 0, 0],  ## under-sample\n",
    "]\n",
    "\n",
    "## conduct Random Over-sampling\n",
    "\n",
    "df_ATOp_RO = iblr.gn(\n",
    "    data = df_ATOp, ## pandas dataframe\n",
    "    y = 'DIAS_MAR',            ## string ('header name')\n",
    "    pert = 0.02,              ## perturbation / noise percentage (pos real) #gaussian\n",
    "    samp_method = 'extreme',   ## string ('balance' or 'extreme')\n",
    "    drop_na_col = True,        ## boolean (True or False)\n",
    "    drop_na_row = True,        ## boolean (True or False)\n",
    "    replace = True,           ## boolean (True or False)\n",
    "    manual_perc = True,      ## user defines percentage of under-sampling and over-sampling  # added\n",
    "    perc_u = 0.9,              ## percentage of under-sampling  # added\n",
    "    perc_o = 100,              ## percentage of over-sampling  # added\n",
    "\n",
    "    ## phi relevance arguments\n",
    "    rel_thres = 0.8,               ## real number (0 < R < 1)\n",
    "    rel_method = 'manual',         ## string ('auto' or 'manual')\n",
    "    # rel_xtrm_type = 'both',      ## unused (rel_method = 'manual')\n",
    "    # rel_coef = 1.50,             ## unused (rel_method = 'manual')\n",
    "    rel_ctrl_pts_rg = rg_mtrx_iblr ## 2d array (format: [x, y])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6q7STsY4WDSp"
   },
   "outputs": [],
   "source": [
    "# Conjuntos de Dados\n",
    "\n",
    "# Atributo Alvo\n",
    "# Conjunto Real\n",
    "y = df_ATOp['DIAS_MAR']\n",
    "# Conjunto Derivado\n",
    "y_s = df_ATOp_RO['DIAS_MAR']\n",
    "\n",
    "# Atrbutos prediditivos exceto colunas eliminadas\n",
    "# Conjunto Real\n",
    "X = df_ATOp.drop(['ANO', 'TRIMESTRE', 'MILHAS', 'DIAS_MAR'], axis=1)\n",
    "# Conjunto Derivado\n",
    "X_s = df_ATOp_RO.drop(['ANO', 'TRIMESTRE', 'MILHAS', 'DIAS_MAR'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PQnn7fPsZnJ"
   },
   "source": [
    "## **Treinamento, Teste, and Validação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros globais\n",
    "\n",
    "N_SPLITS = 10\n",
    "RANDOM_STATE=0\n",
    "KF = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "CV = RepeatedKFold(n_splits=N_SPLITS, n_repeats=5, random_state=RANDOM_STATE)\n",
    "N_TRIALS=50\n",
    "N_RUNS=5  # número de vezes que o modelo roda com colunas embaralhadas\n",
    "N_COLS=2  # número de subconjuntos de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Bloco Experimental 1 - Modelos Individuais**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função unificada de otimização de hiperparâmetros com Optuna\n",
    "def otimizar_modelos_individuais(model_name, X_train, y_train):\n",
    "    def objective(trial):\n",
    "        if model_name == \"LinearRegression\":\n",
    "            model = LinearRegression()\n",
    "\n",
    "        elif model_name == \"DecisionTree\":\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "            }\n",
    "            model = DecisionTreeRegressor(**params, random_state=RANDOM_STATE)\n",
    "\n",
    "        elif model_name == \"RandomForest\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 150),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5)\n",
    "            }\n",
    "            model = RandomForestRegressor(**params, random_state=RANDOM_STATE)\n",
    "\n",
    "        elif model_name == \"GradientBoosting\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 200),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 10)\n",
    "            }\n",
    "            model = GradientBoostingRegressor(**params, random_state=RANDOM_STATE)\n",
    "\n",
    "        elif model_name == \"KNeighbors\":\n",
    "            params = {\n",
    "                'n_neighbors': trial.suggest_int('n_neighbors', 1, 10),\n",
    "                'weights': trial.suggest_categorical('weights', ['uniform', 'distance'])\n",
    "            }\n",
    "            model = KNeighborsRegressor(**params)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo desconhecido: {model_name}\")\n",
    "\n",
    "        return cross_val_score(model, X_train, y_train, cv=KF, scoring='r2', n_jobs=-1).mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=-1)\n",
    "\n",
    "    return study.best_params, study\n",
    "\n",
    "# Execução do Pipeline de otimização dos modelos\n",
    "def executar_pipeline_modelos_individuais(X, y, model_names=None):\n",
    "    if model_names is None:\n",
    "        model_names = [\"LinearRegression\", \"DecisionTree\", \"RandomForest\", \"GradientBoosting\", \"KNeighbors\"]\n",
    "\n",
    "    models_params = {}\n",
    "    study_results = {}\n",
    "    tempos_execucao_total = {}\n",
    "    modelos_treinados = {}\n",
    "\n",
    "    melhor_modelo = None\n",
    "    melhor_score = -np.inf\n",
    "\n",
    "    for name in model_names:\n",
    "        print(f\"\\nOtimizando modelo: {name}\")\n",
    "        inicio = time.perf_counter()\n",
    "\n",
    "        # Otimização\n",
    "        params, study = otimizar_modelos_individuais(name, X, y)\n",
    "        models_params[name] = params\n",
    "        study_results[name] = study\n",
    "\n",
    "        # Criação do modelo com melhores hiperparâmetros\n",
    "        if name == \"LinearRegression\":\n",
    "            modelo = make_pipeline(StandardScaler(), LinearRegression())\n",
    "        elif name == \"DecisionTree\":\n",
    "            modelo = DecisionTreeRegressor(random_state=RANDOM_STATE, **params)\n",
    "        elif name == \"RandomForest\":\n",
    "            modelo = RandomForestRegressor(random_state=RANDOM_STATE, **params)\n",
    "        elif name == \"GradientBoosting\":\n",
    "            modelo = GradientBoostingRegressor(random_state=RANDOM_STATE, **params)\n",
    "        elif name == \"KNeighbors\":\n",
    "            modelo = make_pipeline(StandardScaler(), KNeighborsRegressor(**params))\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo desconhecido: {name}\")\n",
    "\n",
    "        modelos_treinados[name] = modelo\n",
    "\n",
    "        # Avaliação\n",
    "        r2_scores = cross_val_score(modelo, X, y, cv=KF, scoring='r2', n_jobs=-1)\n",
    "        rmse_scores = np.sqrt(-cross_val_score(modelo, X, y, cv=KF, scoring='neg_mean_squared_error', n_jobs=-1))\n",
    "\n",
    "        fim = time.perf_counter()\n",
    "        tempo_total = fim - inicio\n",
    "        tempos_execucao_total[name] = tempo_total\n",
    "\n",
    "        # Armazenamento\n",
    "        media_r2 = np.mean(r2_scores)\n",
    "        models_params[name][\"R2 Média\"] = media_r2\n",
    "        models_params[name][\"R2 Std\"] = np.std(r2_scores)\n",
    "        models_params[name][\"RMSE Média\"] = np.mean(rmse_scores)\n",
    "        models_params[name][\"RMSE Std\"] = np.std(rmse_scores)\n",
    "\n",
    "        # Atualizar melhor modelo\n",
    "        if media_r2 > melhor_score:\n",
    "            melhor_score = media_r2\n",
    "            melhor_modelo = modelo\n",
    "\n",
    "    # Impressão dos resultados\n",
    "    print(\"\\nMelhores hiperparâmetros encontrados:\")\n",
    "    for model, study in study_results.items():\n",
    "        print(f\"{model}: {study.best_params}\")\n",
    "    print(\"\\nResultados e Tempos de Treinamento:\")\n",
    "    for name in model_names:\n",
    "        print(f\"\\n{name}\")\n",
    "        print(f\"Tempo Total: {tempos_execucao_total[name]:.2f} segundos\")\n",
    "        print(f\"Média do R²: {models_params[name]['R2 Média']:.4f} ± {models_params[name]['R2 Std']:.4f}\")\n",
    "        print(f\"Média do RMSE: {models_params[name]['RMSE Média']:.4f} ± {models_params[name]['RMSE Std']:.4f}\")\n",
    "\n",
    "    return models_params, study_results, tempos_execucao_total, melhor_modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bloco Experimental 2 - Comitês de Modelos sem Direcionamento Semântico**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Treinamento de Comitês de Modelos sem Direcionamento Semântico com divisão em dois subconjuntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================\n",
    "# # Classe principal\n",
    "# ================================================\n",
    "class ComiteModelosStacking(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model_names=(\"Linear Regression\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\", \"K-Nearest Neighbors\"),\n",
    "                 model_params=None, meta_model=None, n_runs=N_RUNS, n_cols=N_COLS, random_state=None):\n",
    "        self.model_names = model_names\n",
    "        self.model_params = model_params or {}\n",
    "        self.meta_model = meta_model or Ridge()\n",
    "        self.n_runs = n_runs\n",
    "        self.n_cols = n_cols\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _get_model(self, name, params):\n",
    "        if name == \"Linear Regression\":\n",
    "            return LinearRegression(**params)\n",
    "        elif name == \"Decision Tree\":\n",
    "            return DecisionTreeRegressor(**params)\n",
    "        elif name == \"Random Forest\":\n",
    "            return RandomForestRegressor(**params)\n",
    "        elif name == \"Gradient Boosting\":\n",
    "            return GradientBoostingRegressor(**params)\n",
    "        elif name == \"K-Nearest Neighbors\":\n",
    "            return KNeighborsRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo desconhecido: {name}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.ensembles_ = []\n",
    "        self.cols_used_ = []\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        all_cols = list(X.columns)\n",
    "\n",
    "        for _ in range(self.n_runs):\n",
    "            rng.shuffle(all_cols)\n",
    "            splits = np.array_split(all_cols, self.n_cols)\n",
    "            cols_list = [list(split) for split in splits]\n",
    "            self.cols_used_.append([cols.copy() for cols in cols_list])\n",
    "\n",
    "            estimators = []\n",
    "            for i, (model_name, cols) in enumerate(zip(self.model_names, cols_list)):\n",
    "                prefix = f\"{i}_\"\n",
    "                params = extract_params(self.model_params.get(model_name, {}), prefix)\n",
    "                model = self._get_model(model_name, params)\n",
    "                model.fit(X[cols], y)\n",
    "                estimators.append((f\"model{i+1}\", model))\n",
    "\n",
    "            ensemble = StackingRegressor(\n",
    "                estimators=estimators,\n",
    "                final_estimator=clone(self.meta_model),\n",
    "                passthrough=False\n",
    "            )\n",
    "            ensemble.fit(X[all_cols], y)\n",
    "            self.ensembles_.append((ensemble, [c.copy() for c in cols_list]))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for ensemble, cols_list in self.ensembles_:\n",
    "            used_cols = [col for cols in cols_list for col in cols]\n",
    "            X_used = X[used_cols].copy()\n",
    "            preds.append(ensemble.predict(X_used))\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "# ====================================\n",
    "# Sugestão de hiperparâmetros por modelo\n",
    "# ====================================\n",
    "def suggest_params(trial, model_name, prefix=\"\"):\n",
    "    if model_name == \"Decision Tree\":\n",
    "        return {\n",
    "            f'{prefix}max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10),\n",
    "            f'{prefix}min_samples_split': trial.suggest_int(f'{prefix}min_samples_split', 2, 10),\n",
    "            f'{prefix}min_samples_leaf': trial.suggest_int(f'{prefix}min_samples_leaf', 1, 5)\n",
    "        }\n",
    "    elif model_name == \"Random Forest\":\n",
    "        return {\n",
    "            f'{prefix}n_estimators': trial.suggest_int(f'{prefix}n_estimators', 50, 150),\n",
    "            f'{prefix}max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10),\n",
    "            f'{prefix}min_samples_split': trial.suggest_int(f'{prefix}min_samples_split', 2, 10),\n",
    "            f'{prefix}min_samples_leaf': trial.suggest_int(f'{prefix}min_samples_leaf', 1, 5)\n",
    "        }\n",
    "    elif model_name == \"Gradient Boosting\":\n",
    "        return {\n",
    "            f'{prefix}n_estimators': trial.suggest_int(f'{prefix}n_estimators', 100, 200),\n",
    "            f'{prefix}learning_rate': trial.suggest_float(f'{prefix}learning_rate', 0.01, 0.2),\n",
    "            f'{prefix}max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10)\n",
    "        }\n",
    "    elif model_name == \"K-Nearest Neighbors\":\n",
    "        return {\n",
    "            f'{prefix}n_neighbors': trial.suggest_int(f'{prefix}n_neighbors', 1, 10),\n",
    "            f'{prefix}weights': trial.suggest_categorical(f'{prefix}weights', ['uniform', 'distance'])\n",
    "        }\n",
    "    elif model_name == \"Linear Regression\":\n",
    "        return {}\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo desconhecido: {model_name}\")\n",
    "\n",
    "def extract_params(params_dict, prefix):\n",
    "    return {k.replace(prefix, \"\"): v for k, v in params_dict.items() if k.startswith(prefix)}\n",
    "\n",
    "# ====================================\n",
    "# Treinar dois modelos específicos\n",
    "# ====================================\n",
    "class ComiteModelos2Stacking(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model_names=(\"Gradient Boosting\", \"Linear Regression\"),\n",
    "                 model_params=None, meta_model=None, n_runs=1, random_state=None):\n",
    "        self.model_names = model_names\n",
    "        self.model_params = model_params or {}\n",
    "        self.meta_model = meta_model or Ridge()\n",
    "        self.n_runs = n_runs\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def _get_model(self, name, params):\n",
    "        if name == \"Linear Regression\":\n",
    "            return LinearRegression(**params)\n",
    "        elif name == \"Decision Tree\":\n",
    "            return DecisionTreeRegressor(**params)\n",
    "        elif name == \"Random Forest\":\n",
    "            return RandomForestRegressor(**params)\n",
    "        elif name == \"Gradient Boosting\":\n",
    "            return GradientBoostingRegressor(**params)\n",
    "        elif name == \"K-Nearest Neighbors\":\n",
    "            return KNeighborsRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo desconhecido: {name}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.ensembles_ = []\n",
    "        self.cols_used_ = []\n",
    "        rng = np.random.RandomState(self.random_state)\n",
    "        all_cols = list(X.columns)\n",
    "\n",
    "        for _ in range(self.n_runs):\n",
    "            rng.shuffle(all_cols)\n",
    "            split_idx = len(all_cols) // 2\n",
    "            cols1, cols2 = all_cols[:split_idx], all_cols[split_idx:]\n",
    "\n",
    "            params1 = extract_params(self.model_params.get(self.model_names[0], {}), \"0_\")\n",
    "            params2 = extract_params(self.model_params.get(self.model_names[1], {}), \"1_\")\n",
    "\n",
    "            model1 = self._get_model(self.model_names[0], params1)\n",
    "            model2 = self._get_model(self.model_names[1], params2)\n",
    "\n",
    "            model1.fit(X[cols1], y)\n",
    "            model2.fit(X[cols2], y)\n",
    "\n",
    "            ensemble = StackingRegressor(\n",
    "                estimators=[\n",
    "                    (\"model1\", model1),\n",
    "                    (\"model2\", model2)\n",
    "                ],\n",
    "                final_estimator=clone(self.meta_model),\n",
    "                passthrough=False\n",
    "            )\n",
    "            ensemble.fit(X, y)\n",
    "\n",
    "            self.ensembles_.append((ensemble, cols1, cols2))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = []\n",
    "        for ensemble, cols1, cols2 in self.ensembles_:\n",
    "            X_used = X.copy()\n",
    "            preds.append(ensemble.predict(X_used))\n",
    "        return np.mean(preds, axis=0)\n",
    "\n",
    "# ====================================\n",
    "# Pipeline de execução para todas as combinações de 5 modelos, tomados 2 a 2\n",
    "# ====================================\n",
    "def executar_pipeline_comite_stacking_5modelos(X, y, n_trials=N_TRIALS, n_splits=N_SPLITS, n_runs=N_RUNS, n_cols=N_COLS):\n",
    "    all_model_names = [\n",
    "        \"Linear Regression\",\n",
    "        \"Decision Tree\",\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosting\",\n",
    "        \"K-Nearest Neighbors\"\n",
    "    ]\n",
    "\n",
    "    results = {}\n",
    "    for model_names in permutations(all_model_names, n_cols):\n",
    "        print(f\"\\n Otimizando conjunto com Stacking e 2 divisões de colunas: {model_names}\")\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                name: suggest_params(trial, name, prefix=f\"{i}_\")\n",
    "                for i, name in enumerate(model_names)\n",
    "            }\n",
    "            model = ComiteModelosStacking(\n",
    "                model_names=model_names,\n",
    "                model_params=params,\n",
    "                meta_model=Ridge(),\n",
    "                n_runs=n_runs,\n",
    "                n_cols = n_cols,\n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "\n",
    "            scores = []\n",
    "            for train_idx, val_idx in KF.split(X):\n",
    "                model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "                pred = model.predict(X.iloc[val_idx])\n",
    "                scores.append(r2_score(y.iloc[val_idx], pred))\n",
    "\n",
    "            return np.mean(scores)\n",
    "\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "        best_params = {\n",
    "            name: extract_params(study.best_params, f\"{i}_\")\n",
    "            for i, name in enumerate(model_names)\n",
    "        }\n",
    "\n",
    "        model = ComiteModelosStacking(\n",
    "            model_names=model_names,\n",
    "            model_params={name: best_params[name] for name in model_names},\n",
    "            meta_model=Ridge(),\n",
    "            n_runs=n_runs,\n",
    "            n_cols = n_cols,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "\n",
    "        r2_scores, rmse_scores = [], []\n",
    "        for train_idx, test_idx in KF.split(X):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            pred = model.predict(X.iloc[test_idx])\n",
    "            r2_scores.append(r2_score(y.iloc[test_idx], pred))\n",
    "            rmse_scores.append(root_mean_squared_error(y.iloc[test_idx], pred))\n",
    "\n",
    "        results[\" + \".join(model_names)] = {\n",
    "            \"R2 Média\": np.mean(r2_scores),\n",
    "            \"R2 Std\": np.std(r2_scores),\n",
    "            \"RMSE Média\": np.mean(rmse_scores),\n",
    "            \"RMSE Std\": np.std(rmse_scores),\n",
    "            \"Tempo Total (s)\": study.best_trial.duration.total_seconds()\n",
    "        }\n",
    "\n",
    "    df_results = pd.DataFrame(results).T.sort_values(\"R2 Média\", ascending=False)\n",
    "    print(\"\\n Resultados finais com Stacking e 2 divisões de colunas:\")\n",
    "    print(df_results.round(4))\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Treinamento de Comitês de Modelos sem Direcionamento Semântico com formações completamente aleatórias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para divisão aleatória e desbalanceada de colunas\n",
    "def dividir_colunas_aleatorio_desbalanceado(colunas, n_modelos, rng):\n",
    "    rng.shuffle(colunas)\n",
    "    divisao_inicial = [[coluna] for coluna in colunas[:n_modelos]]\n",
    "    colunas_restantes = colunas[n_modelos:]\n",
    "    for coluna in colunas_restantes:\n",
    "        idx = rng.randint(0, n_modelos)\n",
    "        divisao_inicial[idx].append(coluna)\n",
    "    return divisao_inicial\n",
    "\n",
    "# Pipeline principal\n",
    "def executar_pipeline_execucoes_aleatorias(X, y, n_trials=N_TRIALS, n_splits=N_SPLITS):\n",
    "    all_model_names = [\n",
    "        \"Linear Regression\",\n",
    "        \"Decision Tree\",\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosting\",\n",
    "        \"K-Nearest Neighbors\"\n",
    "    ]\n",
    "\n",
    "    rng_global = np.random.RandomState(RANDOM_STATE)\n",
    "    results = {}\n",
    "\n",
    "    for execucao in range(N_EXECUCOES):\n",
    "        # Sorteia quantos modelos (2, 3, 4 ou 5)\n",
    "        n_modelos = rng_global.choice([2, 3, 4, 5])\n",
    "        # Sorteia os modelos\n",
    "        model_names = rng_global.choice(all_model_names, size=n_modelos, replace=False)\n",
    "        print(f\"\\nExecução {execucao+1}/{N_EXECUCOES}: otimizando conjunto de {n_modelos} modelos -> {model_names}\")\n",
    "\n",
    "        def objective(trial):\n",
    "            params = {\n",
    "                name: suggest_params(trial, name, prefix=f\"{i}_\")\n",
    "                for i, name in enumerate(model_names)\n",
    "            }\n",
    "            model = ComiteModelosStacking(\n",
    "                model_names=model_names,\n",
    "                model_params=params,\n",
    "                meta_model=Ridge(),\n",
    "                n_runs=1,\n",
    "                random_state=rng_global.randint(0, 10000)\n",
    "            )\n",
    "\n",
    "            splits = list(KF.split(X))\n",
    "            scores = []\n",
    "            for train_idx, val_idx in splits:\n",
    "                model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "                pred = model.predict(X.iloc[val_idx])\n",
    "                scores.append(r2_score(y.iloc[val_idx], pred))\n",
    "            return np.mean(scores)\n",
    "\n",
    "        inicio_total = time.perf_counter()\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        fim_total = time.perf_counter()\n",
    "        tempo_total = fim_total - inicio_total\n",
    "\n",
    "        best_params = {\n",
    "            name: extract_params(study.best_params, f\"{i}_\")\n",
    "            for i, name in enumerate(model_names)\n",
    "        }\n",
    "\n",
    "        model = ComiteModelosStacking(\n",
    "            model_names=model_names,\n",
    "            model_params={name: best_params[name] for name in model_names},\n",
    "            meta_model=Ridge(),\n",
    "            n_runs=1,\n",
    "            random_state=rng_global.randint(0, 10000)\n",
    "        )\n",
    "\n",
    "        r2_scores, rmse_scores = [], []\n",
    "        for train_idx, test_idx in KF.split(X):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            pred = model.predict(X.iloc[test_idx])\n",
    "            r2_scores.append(r2_score(y.iloc[test_idx], pred))\n",
    "            rmse_scores.append(root_mean_squared_error(y.iloc[test_idx], pred))\n",
    "\n",
    "        label = f\"Execução {execucao+1}: {' + '.join(model_names)}\"\n",
    "        results[label] = {\n",
    "            \"N Modelos\": n_modelos,\n",
    "            \"Modelos\": \" + \".join(model_names),\n",
    "            \"R2 Média\": np.mean(r2_scores),\n",
    "            \"R2 Std\": np.std(r2_scores),\n",
    "            \"RMSE Média\": np.mean(rmse_scores),\n",
    "            \"RMSE Std\": np.std(rmse_scores),\n",
    "            \"Tempo Total (s)\": tempo_total\n",
    "        }\n",
    "\n",
    "    df_results = pd.DataFrame(results).T.sort_values(\"R2 Média\", ascending=False)\n",
    "    print(\"\\nResultados finais (ordem decrescente de R²):\")\n",
    "    print(df_results.round(4))\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Bloco Experimental 3 - ATOp-Predictive-Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Classe ATOpPredictiveModel\n",
    "# ============================\n",
    "class ATOpPredictiveModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model1_name=\"DecisionTree\", model2_name=\"GradientBoosting\",\n",
    "                 model1_params=None, model2_params=None,\n",
    "                 ensemble_type=\"stacking\", final_estimator=None):\n",
    "        self.model1_name = model1_name\n",
    "        self.model2_name = model2_name\n",
    "        self.model1_params = model1_params or {}\n",
    "        self.model2_params = model2_params or {}\n",
    "        self.ensemble_type = ensemble_type  # 'voting' ou 'stacking'\n",
    "        self.final_estimator = final_estimator or LinearRegression()\n",
    "\n",
    "    def _get_model(self, name, params):\n",
    "        if name == \"LinearRegression\":\n",
    "            return LinearRegression(**params)\n",
    "        elif name == \"DecisionTree\":\n",
    "            return DecisionTreeRegressor(**params)\n",
    "        elif name == \"RandomForest\":\n",
    "            return RandomForestRegressor(**params, random_state=RANDOM_STATE)\n",
    "        elif name == \"GradientBoosting\":\n",
    "            return GradientBoostingRegressor(**params, random_state=RANDOM_STATE)\n",
    "        elif name == \"KNeighbors\":\n",
    "            return KNeighborsRegressor(**params)\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo desconhecido: {name}\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.cols1 = X.loc[:, 'COMBUSTIVEL':'LUBRIFICANTE'].columns\n",
    "        self.cols2 = X.loc[:, 'ACUCARES':'DIAS_PORTO_FORA_SEDE'].columns\n",
    "\n",
    "        self.model1_ = self._get_model(self.model1_name, self.model1_params)\n",
    "        self.model2_ = self._get_model(self.model2_name, self.model2_params)\n",
    "\n",
    "        self.model1_.fit(X[self.cols1], y)\n",
    "        self.model2_.fit(X[self.cols2], y)\n",
    "        \n",
    "        final_estimator = clone(self.final_estimator()) if callable(self.final_estimator) else clone(self.final_estimator)\n",
    "        \n",
    "        if self.ensemble_type == \"voting\":\n",
    "            self.ensemble_ = VotingRegressor([\n",
    "                ('model1', self.model1_),\n",
    "                ('model2', self.model2_)\n",
    "            ])\n",
    "        elif self.ensemble_type == \"stacking\":\n",
    "            self.ensemble_ = StackingRegressor(\n",
    "                estimators=[('model1', self.model1_), ('model2', self.model2_)],\n",
    "                final_estimator=final_estimator,\n",
    "                passthrough=False,\n",
    "                cv=KF\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Tipo de ensemble desconhecido: {self.ensemble_type}\")\n",
    "\n",
    "        self.ensemble_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.ensemble_.predict(X)\n",
    "\n",
    "# ========================\n",
    "# Funções auxiliares\n",
    "# ========================\n",
    "def suggest_params(trial, model_name, prefix=\"\"):\n",
    "    if model_name == \"DecisionTree\":\n",
    "        return {\n",
    "            'max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10),\n",
    "            'min_samples_split': trial.suggest_int(f'{prefix}min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int(f'{prefix}min_samples_leaf', 1, 5)\n",
    "        }\n",
    "    elif model_name == \"RandomForest\":\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int(f'{prefix}n_estimators', 50, 150),\n",
    "            'max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10),\n",
    "            'min_samples_split': trial.suggest_int(f'{prefix}min_samples_split', 2, 10),\n",
    "            'min_samples_leaf': trial.suggest_int(f'{prefix}min_samples_leaf', 1, 5)\n",
    "        }\n",
    "    elif model_name == \"GradientBoosting\":\n",
    "        return {\n",
    "            'n_estimators': trial.suggest_int(f'{prefix}n_estimators', 100, 200),\n",
    "            'learning_rate': trial.suggest_float(f'{prefix}learning_rate', 0.01, 0.2),\n",
    "            'max_depth': trial.suggest_int(f'{prefix}max_depth', 3, 10)\n",
    "        }\n",
    "    elif model_name == \"KNeighbors\":\n",
    "        return {\n",
    "            'n_neighbors': trial.suggest_int(f'{prefix}n_neighbors', 1, 10),\n",
    "            'weights': trial.suggest_categorical(f'{prefix}weights', ['uniform', 'distance'])\n",
    "        }\n",
    "    elif model_name == \"LinearRegression\":\n",
    "        return {}\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo desconhecido: {model_name}\")\n",
    "\n",
    "def extract_params(params_dict, prefix):\n",
    "    return {k.replace(prefix, \"\"): v for k, v in params_dict.items() if k.startswith(prefix)}\n",
    "\n",
    "# =============================================\n",
    "# Pipeline de execução com todas as combinações\n",
    "# =============================================\n",
    "def executar_ATOp_permutacoes(X, y, n_trials=N_TRIALS, n_splits=N_SPLITS, ensemble_type=\"stacking\", final_estimator=None):\n",
    "    model_names = [\"LinearRegression\", \"DecisionTree\", \"RandomForest\", \"GradientBoosting\", \"KNeighbors\"]\n",
    "    results = {}\n",
    "\n",
    "    for name1, name2 in permutations(model_names, 2):\n",
    "        print(f\"\\nOtimizando ATOp com: {name1} + {name2} ({ensemble_type})\")\n",
    "\n",
    "        #inicio_total = time.perf_counter()\n",
    "\n",
    "        def objective(trial):\n",
    "            model1_params = suggest_params(trial, name1, prefix=\"m1_\")\n",
    "            model2_params = suggest_params(trial, name2, prefix=\"m2_\")\n",
    "            model = ATOpPredictiveModel(\n",
    "                model1_name=name1,\n",
    "                model2_name=name2,\n",
    "                model1_params=model1_params,\n",
    "                model2_params=model2_params,\n",
    "                ensemble_type=ensemble_type,\n",
    "                final_estimator=final_estimator\n",
    "            )\n",
    "            scores = []\n",
    "            for train_idx, val_idx in KF.split(X):\n",
    "                model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "                preds = model.predict(X.iloc[val_idx])\n",
    "                scores.append(r2_score(y.iloc[val_idx], preds))\n",
    "            return np.mean(scores)\n",
    "\n",
    "        inicio_total = time.perf_counter()\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        study.optimize(objective, n_trials=n_trials)\n",
    "        fim_total = time.perf_counter()\n",
    "        tempo_total = fim_total - inicio_total\n",
    "\n",
    "        best_params = {\n",
    "            name1: extract_params(study.best_params, \"m1_\"),\n",
    "            name2: extract_params(study.best_params, \"m2_\")\n",
    "        }\n",
    "\n",
    "        model = ATOpPredictiveModel(\n",
    "            model1_name=name1,\n",
    "            model2_name=name2,\n",
    "            model1_params=best_params[name1],\n",
    "            model2_params=best_params[name2],\n",
    "            ensemble_type=ensemble_type,\n",
    "            final_estimator=final_estimator\n",
    "        )\n",
    "\n",
    "        r2_scores = []\n",
    "        rmse_scores = []\n",
    "        for train_idx, test_idx in KF.split(X):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            pred = model.predict(X.iloc[test_idx])\n",
    "            r2_scores.append(r2_score(y.iloc[test_idx], pred))\n",
    "            rmse_scores.append(root_mean_squared_error(y.iloc[test_idx], pred))\n",
    "\n",
    "        #fim_total = time.perf_counter()\n",
    "        #tempo_total = fim_total - inicio_total\n",
    "\n",
    "        results[f\"{name1} + {name2} ({ensemble_type})\"] = {\n",
    "            \"R2 Média\": np.mean(r2_scores),\n",
    "            \"R2 Std\": np.std(r2_scores),\n",
    "            \"RMSE Média\": np.mean(rmse_scores),\n",
    "            \"RMSE Std\": np.std(rmse_scores),\n",
    "            \"Tempo Total (s)\": tempo_total\n",
    "            #\"Tempo Total (s)\": study.best_trial.duration.total_seconds()\n",
    "        }\n",
    "\n",
    "    df_results = pd.DataFrame(results).T.sort_values(\"R2 Média\", ascending=False)\n",
    "    print(\"\\nResultados dos pares no ATOpPredictiveModel:\")\n",
    "    print(df_results.round(4))\n",
    "    return df_results\n",
    "\n",
    "# =============================================\n",
    "# Pipeline de execução para um par específico\n",
    "# =============================================\n",
    "def treinar_ATOp_par(model1_name, model2_name, X, y, n_trials=N_TRIALS, n_splits=N_SPLITS,\n",
    "                     ensemble_type=\"stacking\", final_estimator=None):\n",
    "    print(f\"\\nOtimizando ATOp com: {model1_name} + {model2_name} ({ensemble_type})\")\n",
    "\n",
    "    #inicio_total = time.perf_counter()\n",
    "\n",
    "    def objective(trial):\n",
    "        model1_params = suggest_params(trial, model1_name, prefix=\"m1_\")\n",
    "        model2_params = suggest_params(trial, model2_name, prefix=\"m2_\")\n",
    "        model = ATOpPredictiveModel(\n",
    "            model1_name=model1_name,\n",
    "            model2_name=model2_name,\n",
    "            model1_params=model1_params,\n",
    "            model2_params=model2_params,\n",
    "            ensemble_type=ensemble_type,\n",
    "            final_estimator=final_estimator\n",
    "        )\n",
    "        scores = []\n",
    "        for train_idx, val_idx in KF.split(X):\n",
    "            model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "            preds = model.predict(X.iloc[val_idx])\n",
    "            scores.append(r2_score(y.iloc[val_idx], preds))\n",
    "        return np.mean(scores)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = {\n",
    "        model1_name: extract_params(study.best_params, \"m1_\"),\n",
    "        model2_name: extract_params(study.best_params, \"m2_\")\n",
    "    }\n",
    "\n",
    "    print(\"\\nMelhores hiperparâmetros:\")\n",
    "    print(f\"{model1_name}: {best_params[model1_name]}\")\n",
    "    print(f\"{model2_name}: {best_params[model2_name]}\")\n",
    "\n",
    "    modelo_final = ATOpPredictiveModel(\n",
    "        model1_name=model1_name,\n",
    "        model2_name=model2_name,\n",
    "        model1_params=best_params[model1_name],\n",
    "        model2_params=best_params[model2_name],\n",
    "        ensemble_type=ensemble_type,\n",
    "        final_estimator=final_estimator\n",
    "    )\n",
    "\n",
    "    r2_scores = []\n",
    "    rmse_scores = []\n",
    "    for train_idx, test_idx in KF.split(X):\n",
    "        modelo_final.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
    "        pred = modelo_final.predict(X.iloc[test_idx])\n",
    "        r2_scores.append(r2_score(y.iloc[test_idx], pred))\n",
    "        rmse_scores.append(root_mean_squared_error(y.iloc[test_idx], pred))\n",
    "\n",
    "    #fim_total = time.perf_counter()\n",
    "    #tempo_total = fim_total - inicio_total\n",
    "\n",
    "    print(f\"\\nResultados com {model1_name} + {model2_name} ({ensemble_type}):\")\n",
    "    print(f\"Média do R²: {np.mean(r2_scores):.4f} ± {np.std(r2_scores):.4f}\")\n",
    "    print(f\"Média do RMSE: {np.mean(rmse_scores):.4f} ± {np.std(rmse_scores):.4f}\")\n",
    "    print(f\"Tempo Total: {tempo_total:.2f} segundos\")\n",
    "\n",
    "    return modelo_final, best_params, tempo_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Teste Estatístico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar pasta de saída\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "# ======= DEFINIÇÃO DOS MODELOS =======\n",
    "modelos = {\n",
    "    'Exp1_GB': GradientBoostingRegressor(\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.01,\n",
    "        max_depth=3\n",
    "    ),\n",
    "    'Exp2_Comite': ComiteModelos2Stacking(\n",
    "        model_names=(\"Gradient Boosting\", \"Decision Tree\"),\n",
    "        model_params={\n",
    "            \"Gradient Boosting\": {\"n_estimators\": 100, \"learning_rate\": 0.1, \"max_depth\": 3},\n",
    "            \"Decision Tree\": {\"max_depth\": None, \"min_samples_split\": 2, \"min_samples_leaf\": 1},\n",
    "        },\n",
    "        meta_model=Ridge(),\n",
    "        n_runs=1,\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    'Exp3_ATOp-PM': ATOpPredictiveModel(\n",
    "        model1_name=\"DecisionTree\",\n",
    "        model2_name=\"GradientBoosting\",\n",
    "        model1_params={\"max_depth\": None, \"min_samples_split\": 2, \"min_samples_leaf\": 1},\n",
    "        model2_params={'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3},\n",
    "        ensemble_type=\"stacking\",\n",
    "        final_estimator=Ridge()\n",
    "    )\n",
    "}\n",
    "\n",
    "resultados = []\n",
    "\n",
    "# ======= AVALIAÇÃO CROSS-VALIDATION =======\n",
    "for fold, (train_idx, test_idx) in enumerate(KF.split(X), 1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    for nome, modelo in modelos.items():\n",
    "        start = time.time()\n",
    "        modelo.fit(X_train, y_train)\n",
    "        y_pred = modelo.predict(X_test)\n",
    "        end = time.time()\n",
    "\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        tempo = end - start\n",
    "\n",
    "        resultados.append({\n",
    "            'modelo': nome,\n",
    "            'fold': fold,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'tempo': tempo\n",
    "        })\n",
    "\n",
    "# ======= SALVAR RESULTADOS =======\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "df_resultados.to_csv('outputs/metricas_por_modelo.csv', index=False)\n",
    "# ======= MATRIZES (fold x modelo) =======\n",
    "r2_matrix = df_resultados.pivot(index=\"fold\", columns=\"modelo\", values=\"r2\")\n",
    "r2_matrix.to_csv('outputs/r2_matrix.csv')\n",
    "\n",
    "rmse_matrix = df_resultados.pivot(index=\"fold\", columns=\"modelo\", values=\"rmse\")\n",
    "rmse_matrix.to_csv('outputs/rmse_matrix.csv')\n",
    "\n",
    "tempo_matrix = df_resultados.pivot(index=\"fold\", columns=\"modelo\", values=\"tempo\")\n",
    "tempo_matrix.to_csv('outputs/tempo_matrix.csv')\n",
    "\n",
    "print(\"\\nProcesso concluído. Arquivos gerados na pasta 'outputs'.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "WdgNcg6D-gGj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
